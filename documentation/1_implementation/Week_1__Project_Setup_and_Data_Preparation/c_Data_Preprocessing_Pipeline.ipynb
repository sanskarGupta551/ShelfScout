{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef3bc04-28e1-46c3-8036-0092eaeda23c",
   "metadata": {},
   "source": [
    "# SKU110K Dataset: Implement Data Preprocessing Pipeline for Image Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0bd8cc-029e-43ec-b5bb-11ad26e6f0dd",
   "metadata": {},
   "source": [
    "### 1. Install Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f1111e-022f-4f79-8139-a1330b8ffb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.2.1)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (75.8.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.71.0)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.1.3)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (397 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, tensorboard, keras, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 h5py-3.13.0 keras-3.9.2 libclang-18.1.1 markdown-3.8 ml-dtypes-0.5.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.15.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.0.1 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb67cd0-b108-466e-a194-3cfb374c304b",
   "metadata": {},
   "source": [
    "### 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f03a365d-a5fb-4ed8-8099-7e06a23cdf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 05:50:27.864808: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-18 05:50:28.234203: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-18 05:50:28.527925: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744955428.766803    5536 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744955428.829491    5536 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744955429.422794    5536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744955429.422853    5536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744955429.422855    5536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744955429.422857    5536 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-18 05:50:29.489037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cb3d5-5f1a-4135-81d1-2a9ce1d0636b",
   "metadata": {},
   "source": [
    "### 3. Set Up Dependencies and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a279f783-73a4-4886-898c-8c4161e5952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project configuration\n",
    "PROJECT_ID = \"shelfscout\"\n",
    "BUCKET_NAME = \"sku-110k-dataset\"\n",
    "INPUT_PATH = \"SKU110K_Kaggle\"\n",
    "OUTPUT_PATH = \"processed_data\"\n",
    "TARGET_SIZE = (640, 640)\n",
    "\n",
    "# Initialize storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784e4fe-42ab-4343-9b6e-6bc6b77cbfec",
   "metadata": {},
   "source": [
    "### 4. Funtion for Pre-processing Data and Creating TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b75913-02fd-4897-84e0-c1cd16207c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core preprocessing function\n",
    "def preprocess_and_create_tfrecord(split, num_shards=10):\n",
    "    \"\"\"\n",
    "    Preprocess images and create TFRecords for the given split\n",
    "    \"\"\"\n",
    "    # Create output path\n",
    "    output_dir = f\"{OUTPUT_PATH}/{split}\"\n",
    "    bucket.blob(f\"{output_dir}/\").upload_from_string('')\n",
    "    \n",
    "    # List images\n",
    "    image_blobs = list(bucket.list_blobs(prefix=f\"{INPUT_PATH}/images/{split}/\"))\n",
    "    image_blobs = [blob for blob in image_blobs if blob.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    num_images = len(image_blobs)\n",
    "    \n",
    "    # Calculate sharding\n",
    "    images_per_shard = int(np.ceil(num_images / num_shards))\n",
    "    print(f\"Processing {num_images} images from {split} split into {num_shards} shards\")\n",
    "    \n",
    "    # Process each shard\n",
    "    for shard_id in range(num_shards):\n",
    "        # Set shard range\n",
    "        start_idx = shard_id * images_per_shard\n",
    "        end_idx = min((shard_id + 1) * images_per_shard, num_images)\n",
    "        \n",
    "        # Create TFRecord file locally\n",
    "        output_file = f\"shard_{split}_{shard_id:03d}.tfrecord\"\n",
    "        \n",
    "        with tf.io.TFRecordWriter(output_file) as writer:\n",
    "            # Process each image in shard\n",
    "            for idx in tqdm(range(start_idx, end_idx), desc=f\"Shard {shard_id+1}/{num_shards}\"):\n",
    "                try:\n",
    "                    # Get image data\n",
    "                    image_blob = image_blobs[idx]\n",
    "                    image_name = os.path.basename(image_blob.name)\n",
    "                    image_id = os.path.splitext(image_name)[0]\n",
    "                    \n",
    "                    # Get corresponding annotation\n",
    "                    annotation_path = f\"{INPUT_PATH}/labels/{split}/{image_id}.txt\"\n",
    "                    annotation_blob = bucket.blob(annotation_path)\n",
    "                    \n",
    "                    if not annotation_blob.exists():\n",
    "                        print(f\"Skipping {image_name}: no annotation\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Read image and annotation\n",
    "                    image_data = image_blob.download_as_bytes()\n",
    "                    img = Image.open(io.BytesIO(image_data))\n",
    "                    if img.mode != 'RGB':\n",
    "                        img = img.convert('RGB')\n",
    "                    \n",
    "                    # Read annotation and parse bounding boxes\n",
    "                    annotation_text = annotation_blob.download_as_string().decode('utf-8')\n",
    "                    boxes = []\n",
    "                    for line in annotation_text.strip().split('\\n'):\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 5:\n",
    "                            # Parse according to dataset format\n",
    "                            x1, y1, width, height = map(float, parts[:4])\n",
    "                            x2, y2 = x1 + width, y1 + height\n",
    "                            boxes.append([x1, y1, x2, y2])\n",
    "                    \n",
    "                    # Preprocess image - resize and normalize\n",
    "                    img_resized = img.resize(TARGET_SIZE, Image.LANCZOS)\n",
    "                    img_array = np.array(img_resized, dtype=np.float32) / 255.0\n",
    "                    \n",
    "                    # Scale bounding boxes to new dimensions\n",
    "                    orig_width, orig_height = img.size\n",
    "                    scaled_boxes = []\n",
    "                    for box in boxes:\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        # Convert to absolute pixels in original image\n",
    "                        x1_px = x1 * orig_width\n",
    "                        y1_px = y1 * orig_height\n",
    "                        x2_px = x2 * orig_width\n",
    "                        y2_px = y2 * orig_height\n",
    "                        \n",
    "                        # Scale to new dimensions and normalize\n",
    "                        x1_new = x1_px * TARGET_SIZE[0] / orig_width / TARGET_SIZE[0]\n",
    "                        y1_new = y1_px * TARGET_SIZE[1] / orig_height / TARGET_SIZE[1]\n",
    "                        x2_new = x2_px * TARGET_SIZE[0] / orig_width / TARGET_SIZE[0]\n",
    "                        y2_new = y2_px * TARGET_SIZE[1] / orig_height / TARGET_SIZE[1]\n",
    "                        \n",
    "                        scaled_boxes.append([x1_new, y1_new, x2_new, y2_new])\n",
    "                    \n",
    "                    # Create TF Example\n",
    "                    tf_example = create_tf_example(img_array, scaled_boxes, image_id)\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_name}: {e}\")\n",
    "        \n",
    "        # Upload to GCS\n",
    "        print(f\"Uploading shard {shard_id+1}/{num_shards} to GCS...\")\n",
    "        bucket.blob(f\"{output_dir}/{output_file}\").upload_from_filename(output_file)\n",
    "        os.remove(output_file)\n",
    "    \n",
    "    return f\"gs://{BUCKET_NAME}/{output_dir}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a3920-693d-4d1e-90c4-9a2349c779f0",
   "metadata": {},
   "source": [
    "### 5. Function to generate a metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf6f9ed-5887-455c-a3fd-d364ec9dc084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata file with preprocessing info\n",
    "def create_preprocessing_metadata():\n",
    "    metadata = {\n",
    "        \"dataset\": \"SKU-110K\",\n",
    "        \"preprocessing\": {\n",
    "            \"image_size\": TARGET_SIZE,\n",
    "            \"normalization\": \"0-1 scale\",\n",
    "            \"resize_method\": \"LANCZOS\",\n",
    "            \"format\": \"TFRecord\"\n",
    "        },\n",
    "        \"splits\": {\n",
    "            \"train\": {\"shards\": 10},\n",
    "            \"val\": {\"shards\": 5},\n",
    "            \"test\": {\"shards\": 5}\n",
    "        },\n",
    "        \"created\": \"2025-04-17\",\n",
    "        \"version\": \"1.0\"\n",
    "    }\n",
    "    \n",
    "    # Save metadata locally and to GCS\n",
    "    with open('preprocessing_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    bucket.blob(f\"{OUTPUT_PATH}/metadata.json\").upload_from_filename('preprocessing_metadata.json')\n",
    "    \n",
    "    return \"Metadata created successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508f9a1-1dad-40eb-a80c-a07a3bc69a44",
   "metadata": {},
   "source": [
    "### 6. Executing both Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0867ad-9ca9-470b-97e7-c87db1e69910",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8185 images from train split into 10 shards\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21675c02ddbe4e5e9fbe904e3564517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 1/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 17:09:27.886290: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 1/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a0f2fceaf1410ba86c5e406bae6434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 2/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 2/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8d7f5485db4a588d78c9c97231ac07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 3/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 3/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94febb5fcf17408db121d121573ee69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 4/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 4/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ccfa1cb09c4d24ba448f373b0b1f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 5/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 5/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a563a3817a470d989bc970fe91e878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 6/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 6/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16271c90e6b4401db06970ba6e562f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 7/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 7/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22cfd8009e46c18cae5dbc3a429812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 8/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 8/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5e2f88e7f7497594030839f00d7fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 9/10:   0%|          | 0/819 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 9/10 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a1df6351d14eab98de368e0eb4e1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 10/10:   0%|          | 0/814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 10/10 to GCS...\n",
      "Processed train split: gs://sku-110k-dataset/processed_data/train/\n",
      "Processing 584 images from val split into 5 shards\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088325ee831641d19ddf664038b767e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 1/5:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 1/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574356e80792446ab8eb19a5370858ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 2/5:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 2/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de5eee575624543bd396cb71f0868e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 3/5:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 3/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f2db0554d44970a200943c3e305aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 4/5:   0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 4/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424b72f9e34244278c073ac5f6ef83de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 5/5:   0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 5/5 to GCS...\n",
      "Processed val split: gs://sku-110k-dataset/processed_data/val/\n",
      "Processing 2920 images from test split into 5 shards\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c7a7a6aa4347f9a7fd6a90077a5e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 1/5:   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 1/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ebb27a2581438c86e97b4e24562d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 2/5:   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 2/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01d546ac9ce4b7f84f4f4065503dbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 3/5:   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 3/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a34581aea86499297d0ad01d15f0524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 4/5:   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading shard 4/5 to GCS...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c760ffa2b540480ba2b5cd7d26b48659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shard 5/5:   0%|          | 0/584 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process all splits\n",
    "for split, shards in [(\"train\", 10), (\"val\", 5), (\"test\", 5)]:\n",
    "    output_path = preprocess_and_create_tfrecord(split, shards)\n",
    "    print(f\"Processed {split} split: {output_path}\")\n",
    "\n",
    "# Create metadata\n",
    "create_preprocessing_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e98e68-cd4e-459a-aa41-6b2827623408",
   "metadata": {},
   "source": [
    "### 7. Verifying the Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcd78a-0bb3-498a-8e73-d06e06db8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Verification Section\n",
    "# Add this code to the end of your Data Processing notebook\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def parse_tfrecord_example(example_proto):\n",
    "    \"\"\"Parse a single example from a TFRecord file.\"\"\"\n",
    "    feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/source_id': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
    "        'image/object/class/text': tf.io.VarLenFeature(tf.string),\n",
    "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    \n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Convert sparse tensor to dense\n",
    "    for key in ['image/object/bbox/xmin', 'image/object/bbox/xmax', \n",
    "                'image/object/bbox/ymin', 'image/object/bbox/ymax',\n",
    "                'image/object/class/text', 'image/object/class/label']:\n",
    "        example[key] = tf.sparse.to_dense(example[key])\n",
    "    \n",
    "    return example\n",
    "\n",
    "def visualize_sample(example):\n",
    "    \"\"\"Visualize a sample with its bounding boxes.\"\"\"\n",
    "    # Decode the image\n",
    "    image = tf.io.decode_jpeg(example['image/encoded'])\n",
    "    \n",
    "    # Get image info\n",
    "    height = int(example['image/height'])\n",
    "    width = int(example['image/width'])\n",
    "    filename = example['image/filename'].numpy().decode('utf-8')\n",
    "    \n",
    "    # Get bounding box coordinates\n",
    "    xmins = example['image/object/bbox/xmin'].numpy()\n",
    "    xmaxs = example['image/object/bbox/xmax'].numpy()\n",
    "    ymins = example['image/object/bbox/ymin'].numpy()\n",
    "    ymaxs = example['image/object/bbox/ymax'].numpy()\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image.numpy())\n",
    "    \n",
    "    # Plot bounding boxes (limit to 25 for visibility)\n",
    "    max_boxes = min(25, len(xmins))\n",
    "    for i in range(max_boxes):\n",
    "        # Convert normalized coordinates to actual pixels\n",
    "        xmin, ymin = xmins[i] * width, ymins[i] * height\n",
    "        box_width = (xmaxs[i] - xmins[i]) * width\n",
    "        box_height = (ymaxs[i] - ymins[i]) * height\n",
    "        \n",
    "        # Create rectangle patch\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin), box_width, box_height,\n",
    "            linewidth=1, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    # Add title with information\n",
    "    plt.title(f\"Image: {filename}\\nTotal objects: {len(xmins)} (showing {max_boxes})\")\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if image dimensions match target size\n",
    "    actual_size = image.shape\n",
    "    print(f\"Image dimensions: {actual_size[0]}x{actual_size[1]}\")\n",
    "    print(f\"Expected dimensions: {TARGET_SIZE[1]}x{TARGET_SIZE[0]}\")\n",
    "    \n",
    "    # Check normalization (pixel values should be 0-255 for JPEG encoded images)\n",
    "    decoded = image.numpy()\n",
    "    print(f\"Pixel value range: {decoded.min()} - {decoded.max()}\")\n",
    "    \n",
    "    # Check number of bounding boxes\n",
    "    print(f\"Number of objects: {len(xmins)}\")\n",
    "    \n",
    "    # Display bounding box details for a few boxes\n",
    "    if len(xmins) > 0:\n",
    "        print(\"\\nSample bounding boxes (normalized coordinates):\")\n",
    "        for i in range(min(3, len(xmins))):\n",
    "            print(f\"  Box {i+1}: xmin={xmins[i]:.4f}, ymin={ymins[i]:.4f}, xmax={xmaxs[i]:.4f}, ymax={ymaxs[i]:.4f}\")\n",
    "    \n",
    "    return image.shape, len(xmins)\n",
    "\n",
    "def verify_tfrecords():\n",
    "    \"\"\"Verify TFRecord files from each split.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(f\"\\n{'-'*30}\")\n",
    "        print(f\"Verifying {split} split\")\n",
    "        print(f\"{'-'*30}\")\n",
    "        \n",
    "        # Get list of TFRecord files for this split\n",
    "        blobs = list(bucket.list_blobs(prefix=f\"{OUTPUT_PATH}/{split}/\"))\n",
    "        tfrecord_files = [blob for blob in blobs if blob.name.endswith('.tfrecord')]\n",
    "        \n",
    "        if not tfrecord_files:\n",
    "            print(f\"No TFRecord files found for {split} split\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Found {len(tfrecord_files)} TFRecord files\")\n",
    "        \n",
    "        # Sample a random TFRecord file\n",
    "        sample_file = random.choice(tfrecord_files)\n",
    "        print(f\"Sampling from: {sample_file.name}\")\n",
    "        \n",
    "        # Download the file temporarily\n",
    "        local_file = f\"temp_{split}.tfrecord\"\n",
    "        sample_file.download_to_filename(local_file)\n",
    "        \n",
    "        # Create TFRecord dataset\n",
    "        dataset = tf.data.TFRecordDataset(local_file)\n",
    "        parsed_dataset = dataset.map(parse_tfrecord_example)\n",
    "        \n",
    "        # Count examples and visualize a sample\n",
    "        count = 0\n",
    "        dimensions = []\n",
    "        object_counts = []\n",
    "        \n",
    "        # Count total examples\n",
    "        count = sum(1 for _ in parsed_dataset)\n",
    "        print(f\"Total examples in sampled file: {count}\")\n",
    "        \n",
    "        # Reset the dataset and take a random sample\n",
    "        parsed_dataset = tf.data.TFRecordDataset(local_file).map(parse_tfrecord_example)\n",
    "        samples = parsed_dataset.shuffle(buffer_size=100).take(2)\n",
    "        \n",
    "        # Visualize samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            print(f\"\\nSample {i+1}:\")\n",
    "            dim, obj_count = visualize_sample(sample)\n",
    "            dimensions.append(dim)\n",
    "            object_counts.append(obj_count)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(local_file)\n",
    "        \n",
    "        # Store results\n",
    "        results[split] = {\n",
    "            'file_count': len(tfrecord_files),\n",
    "            'sample_count': count,\n",
    "            'dimensions': dimensions,\n",
    "            'object_counts': object_counts\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run verification\n",
    "verification_results = verify_tfrecords()\n",
    "\n",
    "# Summarize verification results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VERIFICATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for split, result in verification_results.items():\n",
    "    print(f\"\\n{split.upper()} SPLIT:\")\n",
    "    print(f\"  TFRecord files: {result['file_count']}\")\n",
    "    print(f\"  Examples in sampled file: {result['sample_count']}\")\n",
    "    print(f\"  Image dimensions: {[f'{d[0]}x{d[1]}' for d in result['dimensions']]}\")\n",
    "    print(f\"  Objects per image: {result['object_counts']}\")\n",
    "\n",
    "print(\"\\nVerification complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Begin Week 2 model development with the processed data\")\n",
    "print(\"2. Create a baseline model using these TFRecord files\")\n",
    "print(\"3. Set up Vertex AI Experiments for model tracking\")\n",
    "print(f\"\\nProcessed data location: gs://{BUCKET_NAME}/{OUTPUT_PATH}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b6c3f5-f04f-46b6-85fb-ac54bb80e35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
