{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5872ef41-f887-4b11-8b70-2924d1f3ec10",
   "metadata": {},
   "source": [
    "# SKU110k Dataset: Configuring Vertex AI Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d49d5-7111-4d9c-8a30-e22ea570b201",
   "metadata": {},
   "source": [
    "### 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff64b08-efb1-413b-ad27-ad17cb10a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d5768-bc6a-4f3f-8612-774d4c77fc59",
   "metadata": {},
   "source": [
    "### 2. Initialize Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc75bfc-d77c-4c95-be1d-62c41fc12704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project and region\n",
    "PROJECT_ID = \"shelfscout\"\n",
    "REGION = \"us-central1\"     # choose appropriate region\n",
    "BUCKET_NAME = \"sku-110k-dataset\"\n",
    "\n",
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae4263-aa59-4f39-a617-44005021a768",
   "metadata": {},
   "source": [
    "### 3. Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120a8f0e-edce-477b-b874-773811e2ab1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/385790973873/locations/us-central1/datasets/8706296457323347968/operations/7637142448669458432\n",
      "ImageDataset created. Resource name: projects/385790973873/locations/us-central1/datasets/8706296457323347968\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/385790973873/locations/us-central1/datasets/8706296457323347968')\n",
      "Importing ImageDataset data: projects/385790973873/locations/us-central1/datasets/8706296457323347968\n",
      "Import ImageDataset data backing LRO: projects/385790973873/locations/us-central1/datasets/8706296457323347968/operations/352007101444259840\n",
      "ImageDataset data imported. Resource name: projects/385790973873/locations/us-central1/datasets/8706296457323347968\n",
      "Dataset created: projects/385790973873/locations/us-central1/datasets/8706296457323347968\n"
     ]
    }
   ],
   "source": [
    "# Define GCS paths to your data\n",
    "gcs_source = f\"gs://{BUCKET_NAME}/SKU110K_Kaggle\"\n",
    "\n",
    "# Create the Vertex AI Dataset\n",
    "dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"SKU110K-Dataset\",\n",
    "    gcs_source=gcs_source,\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "# Get the created dataset resource name\n",
    "print(f\"Dataset created: {dataset.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c510bb-68db-4f91-b01e-0e2ff7f59516",
   "metadata": {},
   "source": [
    "### 4. Configure Dataset Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "387a2ad0-f3d2-41db-9eba-d6a6e63d14b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/385790973873/locations/us-central1/datasets/4353567417469763584/operations/4925412523038998528\n",
      "ImageDataset created. Resource name: projects/385790973873/locations/us-central1/datasets/4353567417469763584\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/385790973873/locations/us-central1/datasets/4353567417469763584')\n",
      "Importing ImageDataset data: projects/385790973873/locations/us-central1/datasets/4353567417469763584\n",
      "Import ImageDataset data backing LRO: projects/385790973873/locations/us-central1/datasets/4353567417469763584/operations/3311997956533518336\n",
      "ImageDataset data imported. Resource name: projects/385790973873/locations/us-central1/datasets/4353567417469763584\n",
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/385790973873/locations/us-central1/datasets/6668417625938198528/operations/8909690818378334208\n",
      "ImageDataset created. Resource name: projects/385790973873/locations/us-central1/datasets/6668417625938198528\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/385790973873/locations/us-central1/datasets/6668417625938198528')\n",
      "Importing ImageDataset data: projects/385790973873/locations/us-central1/datasets/6668417625938198528\n",
      "Import ImageDataset data backing LRO: projects/385790973873/locations/us-central1/datasets/6668417625938198528/operations/3245288387053092864\n",
      "ImageDataset data imported. Resource name: projects/385790973873/locations/us-central1/datasets/6668417625938198528\n",
      "Creating ImageDataset\n",
      "Create ImageDataset backing LRO: projects/385790973873/locations/us-central1/datasets/3750085067402117120/operations/2118262580303626240\n",
      "ImageDataset created. Resource name: projects/385790973873/locations/us-central1/datasets/3750085067402117120\n",
      "To use this ImageDataset in another session:\n",
      "ds = aiplatform.ImageDataset('projects/385790973873/locations/us-central1/datasets/3750085067402117120')\n",
      "Importing ImageDataset data: projects/385790973873/locations/us-central1/datasets/3750085067402117120\n",
      "Import ImageDataset data backing LRO: projects/385790973873/locations/us-central1/datasets/3750085067402117120/operations/6645506105717817344\n",
      "ImageDataset data imported. Resource name: projects/385790973873/locations/us-central1/datasets/3750085067402117120\n"
     ]
    }
   ],
   "source": [
    "# Create separate datasets for each split\n",
    "train_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"SKU110K-Train\",\n",
    "    gcs_source=f\"gs://{BUCKET_NAME}/SKU110K_Kaggle/images/train\",\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "val_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"SKU110K-Validation\",\n",
    "    gcs_source=f\"gs://{BUCKET_NAME}/SKU110K_Kaggle/images/val\",\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    "    sync=True\n",
    ")\n",
    "\n",
    "test_dataset = aiplatform.ImageDataset.create(\n",
    "    display_name=\"SKU110K-Test\",\n",
    "    gcs_source=f\"gs://{BUCKET_NAME}/SKU110K_Kaggle/images/test\",\n",
    "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.bounding_box,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df827b2-73fa-43fd-9cb8-03c6b323e9dd",
   "metadata": {},
   "source": [
    "### 5. Add Metadata to Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be1425dc-75e1-4b57-bd20-4733ca9356ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated dataset metadata\n"
     ]
    }
   ],
   "source": [
    "# Add metadata to the dataset with compliant labels\n",
    "try:\n",
    "    dataset.update(\n",
    "        labels={\n",
    "            \"purpose\": \"retail_object_detection\",\n",
    "            \"project\": \"shelfscout\",\n",
    "            \"dataset_name\": \"sku110k\",\n",
    "            \"version\": \"1_0\"\n",
    "        }\n",
    "    )\n",
    "    print(\"Successfully updated dataset metadata\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating dataset metadata: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c2304fe-794d-4a4d-b74b-5082724c6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully updated metadata for train dataset\n",
      "Successfully updated metadata for validation dataset\n",
      "Successfully updated metadata for test dataset\n"
     ]
    }
   ],
   "source": [
    "# Add metadata to each split dataset\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    # Get the dataset by name\n",
    "    datasets = aiplatform.ImageDataset.list(\n",
    "        filter=f\"display_name=SKU110K-{split.capitalize()}\"\n",
    "    )\n",
    "    \n",
    "    if datasets:\n",
    "        split_dataset = datasets[0]\n",
    "        \n",
    "        # Add metadata to the split dataset\n",
    "        try:\n",
    "            split_dataset.update(\n",
    "                labels={\n",
    "                    \"purpose\": \"retail_object_detection\",\n",
    "                    \"project\": \"shelfscout\",\n",
    "                    \"dataset_name\": \"sku110k\",\n",
    "                    \"split\": split,\n",
    "                    \"version\": \"1_0\"\n",
    "                }\n",
    "            )\n",
    "            print(f\"Successfully updated metadata for {split} dataset\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating metadata for {split} dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset for split '{split}' not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95492c-c7bb-4fd2-8096-d2b06fa7f4ea",
   "metadata": {},
   "source": [
    "### 6. Verify Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da73ae77-dddb-4b3b-9ea8-2874aa32bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset name: SKU110K-Dataset, resource: projects/385790973873/locations/us-central1/datasets/8706296457323347968\n",
      "Dataset name: SKU110K-Test, resource: projects/385790973873/locations/us-central1/datasets/3750085067402117120\n",
      "Dataset name: SKU110K-Validation, resource: projects/385790973873/locations/us-central1/datasets/6668417625938198528\n",
      "Dataset name: SKU110K-Train, resource: projects/385790973873/locations/us-central1/datasets/4353567417469763584\n"
     ]
    }
   ],
   "source": [
    "# List all datasets to verify creation\n",
    "datasets = aiplatform.ImageDataset.list()\n",
    "for ds in datasets:\n",
    "    print(f\"Dataset name: {ds.display_name}, resource: {ds.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03312c13-788a-4cee-b7ed-926982322041",
   "metadata": {},
   "source": [
    "### 7. Handle Annotation Format Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f8cbff-6dba-48b0-b060-f1c0e8d6edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train annotations...\n",
      "  Processed 0/8185 files\n",
      "  Processed 100/8185 files\n",
      "  Processed 200/8185 files\n",
      "  Processed 300/8185 files\n",
      "  Processed 400/8185 files\n",
      "  Processed 500/8185 files\n",
      "  Processed 600/8185 files\n",
      "  Processed 700/8185 files\n",
      "  Processed 800/8185 files\n",
      "  Processed 900/8185 files\n",
      "  Processed 1000/8185 files\n",
      "  Processed 1100/8185 files\n",
      "  Processed 1200/8185 files\n",
      "  Processed 1300/8185 files\n",
      "  Processed 1400/8185 files\n",
      "  Processed 1500/8185 files\n",
      "  Processed 1600/8185 files\n",
      "  Processed 1700/8185 files\n",
      "  Processed 1800/8185 files\n",
      "  Processed 1900/8185 files\n",
      "  Processed 2000/8185 files\n",
      "  Processed 2100/8185 files\n",
      "  Processed 2200/8185 files\n",
      "  Processed 2300/8185 files\n",
      "  Processed 2400/8185 files\n",
      "  Processed 2500/8185 files\n",
      "  Processed 2600/8185 files\n",
      "  Processed 2700/8185 files\n",
      "  Processed 2800/8185 files\n",
      "  Processed 2900/8185 files\n",
      "  Processed 3000/8185 files\n",
      "  Processed 3100/8185 files\n",
      "  Processed 3200/8185 files\n",
      "  Processed 3300/8185 files\n",
      "  Processed 3400/8185 files\n",
      "  Processed 3500/8185 files\n",
      "  Processed 3600/8185 files\n",
      "  Processed 3700/8185 files\n",
      "  Processed 3800/8185 files\n",
      "  Processed 3900/8185 files\n",
      "  Processed 4000/8185 files\n",
      "  Processed 4100/8185 files\n",
      "  Processed 4200/8185 files\n",
      "  Processed 4300/8185 files\n",
      "  Processed 4400/8185 files\n",
      "  Processed 4500/8185 files\n",
      "  Processed 4600/8185 files\n",
      "  Processed 4700/8185 files\n",
      "  Processed 4800/8185 files\n",
      "  Processed 4900/8185 files\n",
      "  Processed 5000/8185 files\n",
      "  Processed 5100/8185 files\n",
      "  Processed 5200/8185 files\n",
      "  Processed 5300/8185 files\n",
      "  Processed 5400/8185 files\n",
      "  Processed 5500/8185 files\n",
      "  Processed 5600/8185 files\n",
      "  Processed 5700/8185 files\n",
      "  Processed 5800/8185 files\n",
      "  Processed 5900/8185 files\n",
      "  Processed 6000/8185 files\n",
      "  Processed 6100/8185 files\n",
      "  Processed 6200/8185 files\n",
      "  Processed 6300/8185 files\n",
      "  Processed 6400/8185 files\n",
      "  Processed 6500/8185 files\n",
      "  Processed 6600/8185 files\n",
      "  Processed 6700/8185 files\n",
      "  Processed 6800/8185 files\n",
      "  Processed 6900/8185 files\n",
      "  Processed 7000/8185 files\n",
      "  Processed 7100/8185 files\n",
      "  Processed 7200/8185 files\n",
      "  Processed 7300/8185 files\n",
      "  Processed 7400/8185 files\n",
      "  Processed 7500/8185 files\n",
      "  Processed 7600/8185 files\n",
      "  Processed 7700/8185 files\n",
      "  Processed 7800/8185 files\n",
      "  Processed 7900/8185 files\n",
      "  Processed 8000/8185 files\n",
      "  Processed 8100/8185 files\n",
      "Saved 8185 annotations to vertex_annotations/train_annotations.jsonl\n",
      "Uploaded vertex_annotations/train_annotations.jsonl to gs://sku-110k-dataset/vertex_annotations/train_annotations.jsonl\n",
      "Processing val annotations...\n",
      "  Processed 0/584 files\n",
      "  Processed 100/584 files\n",
      "  Processed 200/584 files\n",
      "  Processed 300/584 files\n",
      "  Processed 400/584 files\n",
      "  Processed 500/584 files\n",
      "Saved 584 annotations to vertex_annotations/val_annotations.jsonl\n",
      "Uploaded vertex_annotations/val_annotations.jsonl to gs://sku-110k-dataset/vertex_annotations/val_annotations.jsonl\n",
      "Processing test annotations...\n",
      "  Processed 0/2920 files\n",
      "  Processed 100/2920 files\n",
      "  Processed 200/2920 files\n",
      "  Processed 300/2920 files\n",
      "  Processed 400/2920 files\n",
      "  Processed 500/2920 files\n",
      "  Processed 600/2920 files\n",
      "  Processed 700/2920 files\n",
      "  Processed 800/2920 files\n",
      "  Processed 900/2920 files\n",
      "  Processed 1000/2920 files\n",
      "  Processed 1100/2920 files\n",
      "  Processed 1200/2920 files\n",
      "  Processed 1300/2920 files\n",
      "  Processed 1400/2920 files\n",
      "  Processed 1500/2920 files\n",
      "  Processed 1600/2920 files\n",
      "  Processed 1700/2920 files\n",
      "  Processed 1800/2920 files\n",
      "  Processed 1900/2920 files\n",
      "  Processed 2000/2920 files\n",
      "  Processed 2100/2920 files\n",
      "  Processed 2200/2920 files\n",
      "  Processed 2300/2920 files\n",
      "  Processed 2400/2920 files\n",
      "  Processed 2500/2920 files\n",
      "  Processed 2600/2920 files\n",
      "  Processed 2700/2920 files\n",
      "  Processed 2800/2920 files\n",
      "  Processed 2900/2920 files\n",
      "Saved 2920 annotations to vertex_annotations/test_annotations.jsonl\n",
      "Uploaded vertex_annotations/test_annotations.jsonl to gs://sku-110k-dataset/vertex_annotations/test_annotations.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "# Define a function to convert the annotation format\n",
    "def convert_annotations_to_vertex_format(gcs_bucket_name, annotation_path, image_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert SKU-110K annotations to Vertex AI compatible format (JSONL).\n",
    "    \n",
    "    Args:\n",
    "        gcs_bucket_name: GCS bucket name containing the dataset\n",
    "        annotation_path: Path to annotation files in GCS\n",
    "        image_path: Path to image files in GCS\n",
    "        output_path: Path to save the converted annotation files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a GCS client\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(gcs_bucket_name)\n",
    "    \n",
    "    # Define a function to process a single annotation file\n",
    "    def process_annotation_file(annotation_blob, image_folder):\n",
    "        # Get the annotation file name and corresponding image name\n",
    "        annotation_name = annotation_blob.name.split('/')[-1]\n",
    "        image_name = annotation_name.replace('.txt', '.jpg')\n",
    "        \n",
    "        # Download annotation content\n",
    "        annotation_content = annotation_blob.download_as_string().decode('utf-8')\n",
    "        \n",
    "        # Parse annotation lines\n",
    "        bboxes = []\n",
    "        for line in annotation_content.strip().split('\\n'):\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) >= 5:  # Ensure we have at least class, x1, y1, x2, y2\n",
    "                class_id = int(parts[0])\n",
    "                x1 = float(parts[1])\n",
    "                y1 = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                \n",
    "                # In SKU-110K, coordinates are already normalized (0-1 range)\n",
    "                # and format is [x1 y1 width height confidence]\n",
    "                x2 = x1 + width if width < 1 else width  # Handle different formats\n",
    "                y2 = y1 + height if height < 1 else height\n",
    "                \n",
    "                # Create bbox dictionary\n",
    "                bbox = {\n",
    "                    \"xMin\": x1,\n",
    "                    \"yMin\": y1,\n",
    "                    \"xMax\": x2,\n",
    "                    \"yMax\": y2,\n",
    "                    \"label\": \"product\"  # Assuming all objects are products\n",
    "                }\n",
    "                bboxes.append(bbox)\n",
    "        \n",
    "        # Create Vertex AI compatible annotation\n",
    "        vertex_annotation = {\n",
    "            \"imageGcsUri\": f\"gs://{gcs_bucket_name}/{image_folder}/{image_name}\",\n",
    "            \"boundingBoxAnnotations\": bboxes\n",
    "        }\n",
    "        \n",
    "        return vertex_annotation\n",
    "    \n",
    "    # Process train, val, and test sets\n",
    "    splits = [\"train\", \"val\", \"test\"]\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"Processing {split} annotations...\")\n",
    "        \n",
    "        # List all annotation files for this split\n",
    "        annotation_blobs = list(bucket.list_blobs(prefix=f\"{annotation_path}/{split}/\"))\n",
    "        \n",
    "        vertex_annotations = []\n",
    "        \n",
    "        # Process each annotation file (with progress reporting)\n",
    "        for i, annotation_blob in enumerate(annotation_blobs):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"  Processed {i}/{len(annotation_blobs)} files\")\n",
    "            \n",
    "            try:\n",
    "                vertex_annotation = process_annotation_file(annotation_blob, f\"{image_path}/{split}\")\n",
    "                vertex_annotations.append(vertex_annotation)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {annotation_blob.name}: {e}\")\n",
    "        \n",
    "        # Save annotations to JSONL file\n",
    "        output_file = f\"{output_path}/{split}_annotations.jsonl\"\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            for annotation in vertex_annotations:\n",
    "                f.write(json.dumps(annotation) + '\\n')\n",
    "        \n",
    "        print(f\"Saved {len(vertex_annotations)} annotations to {output_file}\")\n",
    "        \n",
    "        # Upload the JSONL file to GCS\n",
    "        output_blob = bucket.blob(f\"{output_path}/{split}_annotations.jsonl\")\n",
    "        output_blob.upload_from_filename(output_file)\n",
    "        \n",
    "        print(f\"Uploaded {output_file} to gs://{gcs_bucket_name}/{output_path}/{split}_annotations.jsonl\")\n",
    "\n",
    "# Define paths (use variables that should already be defined from previous steps)\n",
    "BUCKET_NAME = \"sku-110k-dataset\"\n",
    "ANNOTATION_PATH = \"SKU110K_Kaggle/labels\"\n",
    "IMAGE_PATH = \"SKU110K_Kaggle/images\"\n",
    "OUTPUT_PATH = \"vertex_annotations\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Call the conversion function\n",
    "convert_annotations_to_vertex_format(BUCKET_NAME, ANNOTATION_PATH, IMAGE_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb850cc-1479-4e0b-8bfb-48380fafdc3d",
   "metadata": {},
   "source": [
    "### 8. Save Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9ac00b0-aeaf-490b-975c-0cf51547a649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the dataset IDs for future reference\n",
    "with open('dataset_info.txt', 'w') as f:\n",
    "    f.write(f\"Main Dataset ID: {dataset.resource_name}\\n\")\n",
    "    f.write(f\"Train Dataset ID: {train_dataset.resource_name}\\n\")\n",
    "    f.write(f\"Validation Dataset ID: {val_dataset.resource_name}\\n\")\n",
    "    f.write(f\"Test Dataset ID: {test_dataset.resource_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ae543-d939-4b65-8b71-c94af57938f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
