{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b4512c-f07c-4ca0-9745-578e098b9c74",
   "metadata": {},
   "source": [
    "# SKU-110K: Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d403b753-5d2a-45e0-8107-8de24cb5f038",
   "metadata": {},
   "source": [
    "### 1. Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6bb669-b300-46f7-b9d1-0c338b268749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import storage\n",
    "import io\n",
    "from PIL import Image\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set up GCP project\n",
    "PROJECT_ID = 'shelfscout'\n",
    "BUCKET_NAME = 'sku-110k-dataset'\n",
    "\n",
    "# Initialize storage client\n",
    "storage_client = storage.Client(project=PROJECT_ID)\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e58303-17fc-4849-aaf4-57e25afbd34e",
   "metadata": {},
   "source": [
    "### 2. Configure Project and Storage Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea5c452-b5e2-4a26-8358-d33863d72d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "844fa15b-e8bb-4229-9952-3ea288cf0d7d",
   "metadata": {},
   "source": [
    "### 3. DATASET DISTRIBUTION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdd7d66-8ae7-4e2a-8f71-39103e24166e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce3ee50d-1680-4f63-875e-9bd85c88868e",
   "metadata": {},
   "source": [
    "### 4. IMAGE PROPERTIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023a657-155e-43db-bb29-73e350648aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample random images from each split for analysis\n",
    "def analyze_image_sample(prefix, sample_size=50):\n",
    "    \"\"\"Analyze properties of a random sample of images\"\"\"\n",
    "    # List all images\n",
    "    blobs = list(bucket.list_blobs(prefix=prefix))\n",
    "    image_blobs = [blob for blob in blobs if not blob.name.endswith('/') \n",
    "                  and blob.name.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Sample images\n",
    "    if len(image_blobs) > sample_size:\n",
    "        sampled_blobs = random.sample(image_blobs, sample_size)\n",
    "    else:\n",
    "        sampled_blobs = image_blobs\n",
    "    \n",
    "    # Properties to collect\n",
    "    widths, heights, aspects = [], [], []\n",
    "    \n",
    "    # Analyze each image\n",
    "    for blob in tqdm(sampled_blobs, desc=f\"Analyzing {prefix}\"):\n",
    "        image_data = blob.download_as_bytes()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "        width, height = img.size\n",
    "        aspect = width / height\n",
    "        \n",
    "        widths.append(width)\n",
    "        heights.append(height)\n",
    "        aspects.append(aspect)\n",
    "    \n",
    "    return {'widths': widths, 'heights': heights, 'aspects': aspects}\n",
    "\n",
    "# Sample from each split and combine (smaller samples for efficiency)\n",
    "print(\"Analyzing image properties (this may take a minute)...\")\n",
    "train_props = analyze_image_sample(train_path, sample_size=50)\n",
    "val_props = analyze_image_sample(val_path, sample_size=25)\n",
    "test_props = analyze_image_sample(test_path, sample_size=50)\n",
    "\n",
    "# Combine properties\n",
    "widths = train_props['widths'] + val_props['widths'] + test_props['widths']\n",
    "heights = train_props['heights'] + val_props['heights'] + test_props['heights']\n",
    "aspects = train_props['aspects'] + val_props['aspects'] + test_props['aspects']\n",
    "\n",
    "# Calculate key statistics\n",
    "width_mean, width_std = np.mean(widths), np.std(widths)\n",
    "height_mean, height_std = np.mean(heights), np.std(heights)\n",
    "aspect_mean, aspect_std = np.mean(aspects), np.std(aspects)\n",
    "\n",
    "# Display key image statistics\n",
    "print(\"\\nImage Property Statistics:\")\n",
    "print(f\"Resolution: {width_mean:.1f}×{height_mean:.1f} pixels (mean)\")\n",
    "print(f\"Width range: {min(widths)}-{max(widths)} pixels\")\n",
    "print(f\"Height range: {min(heights)}-{max(heights)} pixels\")\n",
    "print(f\"Aspect ratio: {aspect_mean:.2f} (mean), {min(aspects):.2f}-{max(aspects):.2f} (range)\")\n",
    "\n",
    "# Resolution visualization\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(widths, heights, alpha=0.5)\n",
    "plt.axvline(width_mean, color='r', linestyle='--', label=f'Mean width: {width_mean:.1f}px')\n",
    "plt.axhline(height_mean, color='g', linestyle='--', label=f'Mean height: {height_mean:.1f}px')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.title('Image Resolution Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e2e21-775d-417c-997b-d4fd5a1731b9",
   "metadata": {},
   "source": [
    "### 5. ANNOTATION ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba554d5-8f39-4ce4-8d74-77b1a0265b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_annotations_sample(labels_path, sample_size=50):\n",
    "    \"\"\"Analyze annotations for a sample of images\"\"\"\n",
    "    # List all annotation files\n",
    "    blobs = list(bucket.list_blobs(prefix=labels_path))\n",
    "    label_blobs = [blob for blob in blobs if not blob.name.endswith('/')]\n",
    "    \n",
    "    # Sample annotations\n",
    "    if len(label_blobs) > sample_size:\n",
    "        sampled_blobs = random.sample(label_blobs, sample_size)\n",
    "    else:\n",
    "        sampled_blobs = label_blobs\n",
    "    \n",
    "    # Stats to collect\n",
    "    objects_per_image = []\n",
    "    bbox_areas = []\n",
    "    \n",
    "    # Analyze each annotation file\n",
    "    for blob in tqdm(sampled_blobs, desc=f\"Analyzing {labels_path}\"):\n",
    "        content = blob.download_as_text()\n",
    "        lines = content.strip().split('\\n')\n",
    "        \n",
    "        # Count objects\n",
    "        num_objects = len(lines)\n",
    "        objects_per_image.append(num_objects)\n",
    "        \n",
    "        # Process each bounding box\n",
    "        for line in lines:\n",
    "            try:\n",
    "                parts = line.strip().split()\n",
    "                # SKU-110K format is typically: x1 y1 width height confidence\n",
    "                x1, y1, width, height = map(float, parts[:4])\n",
    "                \n",
    "                # Calculate area (normalized)\n",
    "                area = width * height\n",
    "                bbox_areas.append(area)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line: {line}, {e}\")\n",
    "                continue\n",
    "    \n",
    "    return {\n",
    "        'objects_per_image': objects_per_image,\n",
    "        'bbox_areas': bbox_areas\n",
    "    }\n",
    "\n",
    "# Analyze annotations from each split\n",
    "print(\"\\nAnalyzing annotations...\")\n",
    "train_annot = analyze_annotations_sample('SKU110K_Kaggle/labels/train/', sample_size=50)\n",
    "val_annot = analyze_annotations_sample('SKU110K_Kaggle/labels/val/', sample_size=25)\n",
    "test_annot = analyze_annotations_sample('SKU110K_Kaggle/labels/test/', sample_size=50)\n",
    "\n",
    "# Combine results\n",
    "all_objects_per_image = train_annot['objects_per_image'] + val_annot['objects_per_image'] + test_annot['objects_per_image']\n",
    "all_bbox_areas = train_annot['bbox_areas'] + val_annot['bbox_areas'] + test_annot['bbox_areas']\n",
    "\n",
    "# Calculate key statistics\n",
    "obj_mean = np.mean(all_objects_per_image)\n",
    "obj_median = np.median(all_objects_per_image)\n",
    "obj_min, obj_max = min(all_objects_per_image), max(all_objects_per_image)\n",
    "\n",
    "area_mean = np.mean(all_bbox_areas)\n",
    "area_median = np.median(all_bbox_areas)\n",
    "\n",
    "# Display key annotation statistics\n",
    "print(\"\\nAnnotation Statistics:\")\n",
    "print(f\"Objects per image: {obj_mean:.1f} (mean), {obj_median:.1f} (median)\")\n",
    "print(f\"Object count range: {obj_min}-{obj_max} objects per image\")\n",
    "print(f\"Average bounding box area: {area_mean:.6f} (normalized)\")\n",
    "print(f\"Total objects analyzed: {len(all_bbox_areas)}\")\n",
    "\n",
    "# Visualize objects per image distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(all_objects_per_image, bins=20, kde=True)\n",
    "plt.axvline(obj_mean, color='r', linestyle='--', label=f'Mean: {obj_mean:.1f}')\n",
    "plt.axvline(obj_median, color='g', linestyle='--', label=f'Median: {obj_median:.1f}')\n",
    "plt.xlabel('Objects per Image')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Objects per Image')\n",
    "plt.legend()\n",
    "\n",
    "# Visualize bounding box area distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(all_bbox_areas, bins=20, kde=True, log_scale=(False, True))\n",
    "plt.axvline(area_mean, color='r', linestyle='--', label=f'Mean: {area_mean:.6f}')\n",
    "plt.axvline(area_median, color='g', linestyle='--', label=f'Median: {area_median:.6f}')\n",
    "plt.xlabel('Bounding Box Area (normalized)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Bounding Box Area Distribution')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408c83c7-4696-4e37-a6c0-177fd64f530c",
   "metadata": {},
   "source": [
    "### 6. SUMMARY REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad44d39-313d-49e5-93df-e185a5585869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary for model development\n",
    "summary = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Images',\n",
    "        'Training Split',\n",
    "        'Validation Split',\n",
    "        'Test Split',\n",
    "        'Mean Image Resolution',\n",
    "        'Resolution Range',\n",
    "        'Mean Aspect Ratio',\n",
    "        'Mean Objects per Image',\n",
    "        'Objects per Image Range',\n",
    "        'Mean Bounding Box Area',\n",
    "        'Model Target Resolution',\n",
    "        'Required Preprocessing'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{total_images}\",\n",
    "        f\"{train_count} images ({train_percent:.1f}%)\",\n",
    "        f\"{val_count} images ({val_percent:.1f}%)\",\n",
    "        f\"{test_count} images ({test_percent:.1f}%)\",\n",
    "        f\"{width_mean:.1f}×{height_mean:.1f} pixels\",\n",
    "        f\"{min(widths)}×{min(heights)} to {max(widths)}×{max(heights)} pixels\",\n",
    "        f\"{aspect_mean:.2f} (range: {min(aspects):.2f}-{max(aspects):.2f})\",\n",
    "        f\"{obj_mean:.1f} (median: {obj_median:.1f})\",\n",
    "        f\"{obj_min}-{obj_max} objects\",\n",
    "        f\"{area_mean:.6f} (normalized)\",\n",
    "        \"640×640 pixels (recommended)\",\n",
    "        \"Resize maintaining aspect ratio + normalization\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nSKU-110K Dataset Analysis Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(summary.to_string(index=False))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed732c-337a-4d04-9891-387012857c6c",
   "metadata": {},
   "source": [
    "### 7. MODEL DEVELOPMENT INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6268a74f-f79f-4f20-ae4b-84f2662e844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nKey Insights for Model Development:\")\n",
    "print(\"1. High object density (avg. %d objects per image) suggests:\" % obj_mean)\n",
    "print(\"   - Need for high-resolution input to distinguish small objects\")\n",
    "print(\"   - Model architecture with strong small-object detection capability\")\n",
    "print(\"   - Potential for anchor optimization for small, densely packed objects\")\n",
    "\n",
    "print(\"\\n2. Resolution considerations:\")\n",
    "print(\"   - Significant variation in image sizes requires consistent resizing\")\n",
    "print(\"   - Recommend 640×640 target size (standard for object detection)\")\n",
    "print(\"   - Must preserve aspect ratio during resizing to avoid distortion\")\n",
    "\n",
    "print(\"\\n3. Data distribution:\")\n",
    "print(\"   - Strong train/val/test split already provided (70/5/25)\")\n",
    "print(\"   - No need for additional splitting\")\n",
    "print(\"   - Sufficient training data (%d images)\" % train_count)\n",
    "\n",
    "print(\"\\n4. Preprocessing requirements:\")\n",
    "print(\"   - Resize images to 640×640 while maintaining aspect ratio\")\n",
    "print(\"   - Apply padding where necessary\")\n",
    "print(\"   - Normalize pixel values to [0-1] range\")\n",
    "print(\"   - Convert to TFRecord format for efficient training\")\n",
    "\n",
    "# Save this report\n",
    "summary.to_csv('sku110k_dataset_summary.csv', index=False)\n",
    "print(\"\\nSummary saved to 'sku110k_dataset_summary.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0f6e1b-3176-4e33-a1fa-d2c41883e5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
